{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import line_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x:np.ndarray):\n",
    "#   Define the object function\n",
    "    return 100.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_rosenbrock(x:np.ndarray):\n",
    "#   Gradient of the Rosenbrock function.\n",
    "\n",
    "    x_val, y_val = x[0], x[1]\n",
    "    df_dx = -400.0 * x_val * (y_val - x_val**2) - 2.0 * (1.0 - x_val)\n",
    "    df_dy =  200.0 * (y_val - x_val**2)\n",
    "    return np.array([df_dx, df_dy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "\n",
    "    return x[0] + x[1] - 1.0\n",
    "\n",
    "def grad_h(x):\n",
    "\n",
    "    return np.array([1.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_lagrangian(x, lam, rho):\n",
    "\n",
    "    return rosenbrock(x) + lam*h(x) + 0.5*rho*(h(x)**2)\n",
    "\n",
    "def grad_aug_lagrangian(x, lam, rho):\n",
    "\n",
    "    return grad_rosenbrock(x) + lam*grad_h(x) + rho*h(x)*grad_h(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(x, p, grad, func, c=1e-4, rho=0.8):\n",
    "\n",
    "    alpha = 1.0\n",
    "    fx = func(x)\n",
    "    while True:\n",
    "        x_new = x + alpha * p\n",
    "        lhs = func(x_new)\n",
    "        rhs = fx + c * alpha * np.dot(grad, p)\n",
    "        if lhs <= rhs:\n",
    "            break\n",
    "        alpha *= rho\n",
    "        if alpha < 1e-16:\n",
    "            break\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs(x0:np.ndarray, lam, rho, tol=1e-7, max_iter=100000):\n",
    "\n",
    "    n = len(x0)\n",
    "    x = x0\n",
    "    B = np.eye(n)  # initial inverse Hessian surrogate\n",
    "    iter = 0\n",
    "\n",
    "    while iter < max_iter:\n",
    "        grad_L = grad_aug_lagrangian(x, lam, rho)\n",
    "        if np.linalg.norm(grad_L) < tol:\n",
    "            break\n",
    "        \n",
    "        p = -np.linalg.solve(B, grad_L) \n",
    "        alpha = backtracking_line_search(x, p, grad_L,\n",
    "                                         lambda z: aug_lagrangian(z, lam, rho))\n",
    "\n",
    "        s = alpha * p\n",
    "        x_new = x + s\n",
    "        grad_L_new = grad_aug_lagrangian(x_new, lam, rho)\n",
    "        y = grad_L_new - grad_L\n",
    "        # Curvature condition\n",
    "        if np.dot(y, s) > 1e-10:\n",
    "            d1 = np.inner(s, B @ s)\n",
    "            d2 = np.inner(y, s)\n",
    "            B = B - (1/d1) * np.outer(B@s, s@B) + (1/d2) * np.outer(y, y)\n",
    "\n",
    "        x = x_new\n",
    "        iter += 1\n",
    "        \n",
    "    return x, iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_lagrangian_method(x0, tol=1e-6, outer_it=20):\n",
    "    \"\"\"\n",
    "    Solve the constrained problem:\n",
    "        min f(x)\n",
    "        s.t. g(x) = 0\n",
    "    using the Augmented Lagrangian approach:\n",
    "      L(x; lam, rho) = f(x) + lam*g(x) + (rho/2)*[g(x)]^2.\n",
    "    Use BFGS to solve each subproblem.\n",
    "    \"\"\"\n",
    "    # Initialize Lagrange rholtiplier and penalty\n",
    "    xk = x0\n",
    "    lam = 0.0\n",
    "    rho = 1.0\n",
    "    \n",
    "    total_iter = 0\n",
    "    for outer_iter in range(outer_it):\n",
    "    \n",
    "        xk, bfgs_its = bfgs(xk, lam, rho, tol=tol)\n",
    "        total_iter += bfgs_its\n",
    "        \n",
    "        # primal feasibility\n",
    "        primal_resid = abs(h(xk))    \n",
    "        # Since there is no inequality constraints, no need of dual feasibility\n",
    "        \n",
    "        if primal_resid < tol:\n",
    "            break\n",
    "        \n",
    "        # Standard augmented Lagrangian update\n",
    "        lam = lam + rho*h(xk)\n",
    "        \n",
    "        # Increase penalty term by \\gamma=10\n",
    "        if primal_resid > tol:\n",
    "            rho *= 10.0\n",
    "    \n",
    "    return xk, lam, outer_iter+1, total_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Lagrangian Method Results\n",
      "------------------------------------\n",
      "Starting point:        [-1.  1.]\n",
      "Optimal solution x*:   [0.61879563 0.3812044 ]\n",
      "Optimal Lagrange λ*:   0.340698\n",
      "Outer iterations:      4\n",
      "Total BFGS iterations: 53\n",
      "Constraint value h(x*)= 2.916340e-08\n",
      "Final objective f(x*) = 1.456070e-01\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.0, 1.0])\n",
    "    \n",
    "# Solve with Augmented Lagrangian\n",
    "x_opt, lam_opt, outer_iters, total_bfgs_its = augmented_lagrangian_method(x0, tol=1e-6)\n",
    "\n",
    "# Print results\n",
    "print(\"Augmented Lagrangian Method Results\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"Starting point:        {x0}\")\n",
    "print(f\"Optimal solution x*:   {x_opt}\")\n",
    "print(f\"Optimal Lagrange λ*:   {lam_opt:.6f}\")\n",
    "print(f\"Outer iterations:      {outer_iters}\")\n",
    "print(f\"Total BFGS iterations: {total_bfgs_its}\")\n",
    "print(f\"Constraint value h(x*)= {h(x_opt):.6e}\")\n",
    "print(f\"Final objective f(x*) = {rosenbrock(x_opt):.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of increasing penalty term is to approximate $\\rho \\to \\inf $, so that solution approach feasible region."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
