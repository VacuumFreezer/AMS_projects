{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02283f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import ale_py\n",
    "import imageio\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register ALE environments\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "NUM_ENVS = 16\n",
    "\n",
    "# Note: Your TF code actually used 500 steps (Line 8 of your TF script).\n",
    "# However, PPO performs better with 2048. We use 2048 here.\n",
    "# To handle the memory load of 2048 * 210x160x3 images, we use pre-allocated arrays.\n",
    "NUM_STEPS = 1024        \n",
    "\n",
    "NUM_GENS = 100\n",
    "LEARNING_RATE = 1.0e-4 \n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPS = 0.1         \n",
    "BATCH_SIZE = 32        \n",
    "UPDATE_EPOCHS = 3      \n",
    "ENTROPY_COEF = 0.01\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# os.makedirs(\"ppo_tf_checkpoints\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Dataset for Memory Efficiency ---\n",
    "class AtariDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Keeps data in uint8 (CPU) to save RAM.\n",
    "    Converts to float32 (GPU) only when a batch is requested.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs, act, lps, adv, ret, device):\n",
    "        self.obs = obs # uint8 numpy array\n",
    "        self.act = torch.tensor(act, dtype=torch.long, device=device)\n",
    "        self.lps = torch.tensor(lps, dtype=torch.float32, device=device)\n",
    "        self.adv = torch.tensor(adv, dtype=torch.float32, device=device)\n",
    "        self.ret = torch.tensor(ret, dtype=torch.float32, device=device)\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.obs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert single item/slice to float32 and move to device on the fly\n",
    "        # (B, H, W, C) -> (B, C, H, W)\n",
    "        o = self.obs[idx]\n",
    "        o_tensor = torch.tensor(o, dtype=torch.float32, device=self.device)\n",
    "        o_tensor = o_tensor.permute(2, 0, 1) / 255.0\n",
    "        \n",
    "        return o_tensor, self.act[idx], self.lps[idx], self.adv[idx], self.ret[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Models (Replicating the TF 5-Layer Architecture) ---\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(Actor, self).__init__()\n",
    "        # TF Architecture: 5 Conv layers taking 210x160x3 input\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2), nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 210, 160)\n",
    "            flat_size = self.features(dummy).shape[1]\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(flat_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_actions)\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
    "            module.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb101691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2), nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 210, 160)\n",
    "            flat_size = self.features(dummy).shape[1]\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(flat_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
    "            module.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8bac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Rollout Logic ---\n",
    "def rollout(env, p_model, v_model, num_steps, last_obs=None, device='cpu'):\n",
    "    if last_obs is None:\n",
    "        obs, info = env.reset()\n",
    "    else:\n",
    "        obs = last_obs\n",
    "\n",
    "    # Pre-allocate buffers to prevent memory fragmentation and spikes\n",
    "    # Shape: (Steps, Envs, H, W, C) for Obs\n",
    "    obs_buf = np.zeros((num_steps, NUM_ENVS, 210, 160, 3), dtype=np.uint8)\n",
    "    act_buf = np.zeros((num_steps, NUM_ENVS), dtype=np.int32)\n",
    "    lp_buf  = np.zeros((num_steps, NUM_ENVS), dtype=np.float32)\n",
    "    val_buf = np.zeros((num_steps, NUM_ENVS), dtype=np.float32)\n",
    "    rew_buf = np.zeros((num_steps, NUM_ENVS), dtype=np.float32)\n",
    "    done_buf = np.zeros((num_steps, NUM_ENVS), dtype=np.float32)\n",
    "\n",
    "    p_model.eval()\n",
    "    v_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(num_steps):\n",
    "            # Normalize just for the forward pass\n",
    "            obs_norm = obs.astype(np.float32) / 255.0\n",
    "            obs_tensor = torch.tensor(obs_norm, dtype=torch.float32, device=device).permute(0, 3, 1, 2)\n",
    "            \n",
    "            logits = p_model(obs_tensor)\n",
    "            dist = Categorical(logits=logits)\n",
    "            actions = dist.sample()\n",
    "            log_probs = dist.log_prob(actions)\n",
    "            values = v_model(obs_tensor).squeeze(-1)\n",
    "\n",
    "            nxt, reward, terminated, truncated, _ = env.step(actions.cpu().numpy())\n",
    "            dones = np.logical_or(terminated, truncated).astype(np.float32)\n",
    "\n",
    "            # Store RAW reward here for logging\n",
    "            # We will clip it later for training\n",
    "            obs_buf[step] = obs\n",
    "            act_buf[step] = actions.cpu().numpy()\n",
    "            lp_buf[step]  = log_probs.cpu().numpy()\n",
    "            val_buf[step] = values.cpu().numpy()\n",
    "            rew_buf[step] = reward\n",
    "            done_buf[step] = dones\n",
    "\n",
    "            obs = nxt\n",
    "\n",
    "    return obs_buf, act_buf, lp_buf, val_buf, rew_buf, done_buf, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e538fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. GAE Calculation ---\n",
    "def compute_gae(values, rewards, dones, next_value, gamma, lam):\n",
    "    advantages = np.zeros_like(rewards)\n",
    "    last_gae_lam = 0\n",
    "    values_ext = np.append(values, next_value[None, :], axis=0)\n",
    "    \n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * values_ext[t+1] * (1 - dones[t]) - values_ext[t]\n",
    "        last_gae_lam = delta + gamma * lam * (1 - dones[t]) * last_gae_lam\n",
    "        advantages[t] = last_gae_lam\n",
    "        \n",
    "    returns = advantages + values\n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c471d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Training Step ---\n",
    "def train_step(p_model, v_model, p_opt, v_opt, batch):\n",
    "    obs, act, old_lps, adv, ret = batch\n",
    "    \n",
    "    # --- Update Actor ---\n",
    "    p_model.train()\n",
    "    logits = p_model(obs)\n",
    "    dist = Categorical(logits=logits)\n",
    "    new_lps = dist.log_prob(act)\n",
    "    entropy = dist.entropy().mean()\n",
    "\n",
    "    ratio = torch.exp(new_lps - old_lps)\n",
    "    surr1 = ratio * adv\n",
    "    surr2 = torch.clamp(ratio, 1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * adv\n",
    "    ppo_loss = -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "    policy_loss = ppo_loss - ENTROPY_COEF * entropy\n",
    "\n",
    "    p_opt.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    nn.utils.clip_grad_norm_(p_model.parameters(), 1.0)\n",
    "    p_opt.step()\n",
    "\n",
    "    # --- Update Critic ---\n",
    "    v_model.train()\n",
    "    values = v_model(obs).squeeze(-1)\n",
    "    value_loss = ((ret - values) ** 2).mean()\n",
    "    \n",
    "    v_opt.zero_grad()\n",
    "    value_loss.backward()\n",
    "    nn.utils.clip_grad_norm_(v_model.parameters(), 1.0)\n",
    "    v_opt.step()\n",
    "\n",
    "    return ppo_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main ---\n",
    "def main():\n",
    "    print(f\"Optimized PPO Implementation A on {DEVICE}\")\n",
    "    # Using mixed precision scaler\n",
    "    \n",
    "    env = gym.make_vec('ALE/Galaxian-v5', frameskip=4, num_envs=NUM_ENVS, vectorization_mode='sync')\n",
    "    \n",
    "    p_model = Actor(env.single_action_space.n).to(DEVICE)\n",
    "    v_model = Critic().to(DEVICE)\n",
    "    p_optimizer = optim.Adam(p_model.parameters(), lr=LEARNING_RATE)\n",
    "    v_optimizer = optim.Adam(v_model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    current_obs = None\n",
    "    # Logging Lists for Plotting\n",
    "    avg_scores_history = []\n",
    "    max_scores_history = []\n",
    "\n",
    "    for gen in range(NUM_GENS):\n",
    "        # 1. Rollout\n",
    "        obs, act, lps, vals, raw_rew, dones, current_obs = rollout(\n",
    "            env, p_model, v_model, NUM_STEPS, last_obs=current_obs, device=DEVICE\n",
    "        )\n",
    "\n",
    "        # 2. Prep\n",
    "        clipped_rew = np.sign(raw_rew)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            o_norm = current_obs.astype(np.float32) / 255.0\n",
    "            last_obs_tensor = torch.tensor(o_norm, dtype=torch.float32, device=DEVICE).permute(0, 3, 1, 2)\n",
    "            next_val = v_model(last_obs_tensor).squeeze(-1).cpu().numpy()\n",
    "\n",
    "        adv, ret = compute_gae(vals, clipped_rew, dones, next_val, GAMMA, LAMBDA)\n",
    "        adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "        # 3. Dataset\n",
    "        b_obs = obs.reshape(-1, 210, 160, 3)\n",
    "        b_act = act.reshape(-1)\n",
    "        b_lps = lps.reshape(-1)\n",
    "        b_adv = adv.reshape(-1)\n",
    "        b_ret = ret.reshape(-1)\n",
    "\n",
    "        dataset = AtariDataset(b_obs, b_act, b_lps, b_adv, b_ret, DEVICE)\n",
    "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        # 4. Train\n",
    "        metrics = []\n",
    "        for _ in range(UPDATE_EPOCHS):\n",
    "            for batch in dataloader:\n",
    "                loss = train_step(p_model, v_model, p_optimizer, v_optimizer, batch)\n",
    "                metrics.append(loss)\n",
    "        \n",
    "        avg_loss = np.mean(metrics)\n",
    "        \n",
    "        # Log only essential info (Reward & PPO Loss)\n",
    "        total_batch_reward = np.sum(raw_rew, axis=0) # Shape: (16,)\n",
    "        avg_batch_reward = np.mean(total_batch_reward)\n",
    "        max_batch_reward = np.max(total_batch_reward)\n",
    "\n",
    "        avg_scores_history.append(avg_batch_reward)\n",
    "        max_scores_history.append(max_batch_reward)\n",
    "        \n",
    "        # Use carriage return \\r to update line in place if desired, or standard print\n",
    "        print(f\"Gen {gen:3d} | Avg Score: {avg_batch_reward:6.1f} | PPO Loss: {avg_loss:.4f}\")\n",
    "\n",
    "   # Save\n",
    "    torch.save(p_model.state_dict(), \"ppo_atari_translated_actor.pth\")\n",
    "    torch.save(v_model.state_dict(), \"ppo_atari_translated_critic.pth\")\n",
    "    \n",
    "    # --- Generate Plot ---\n",
    "    print(\"Generating Training Plot...\")\n",
    "    epochs = range(1, NUM_GENS + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Left Subplot: Average Score\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, avg_scores_history, '*-b') # Blue line with stars\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('score')\n",
    "    plt.title('Average Score per Epoch')\n",
    "    \n",
    "    # Right Subplot: Max Score\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, max_scores_history, '--r') # Red dashed line\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Max score')\n",
    "    plt.title('Maximum Score per Epoch')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_plot.png')\n",
    "    print(\"Plot saved as 'training_plot.png'\")\n",
    "    \n",
    "    # Render\n",
    "    print(\"Rendering...\")\n",
    "    obs, _, _, _, _, _, _ = rollout(env, p_model, v_model, num_steps=1024, last_obs=None, device=DEVICE)\n",
    "    to_render = obs[:, 0, ...]\n",
    "    imageio.mimsave('ppo_atari_translated.gif', to_render, fps=30, loop=0)\n",
    "\n",
    "    print(\"Saved ppo_opt_result.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b7ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceacf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register ALE environments\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# --- Configuration ---\n",
    "ENV_NAME = \"ALE/Galaxian-v5\"\n",
    "NUM_ENVS = 16\n",
    "GIF_STEPS = 1024\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ACTOR_PATH = \"ppo_atari_translated_actor.pth\"\n",
    "\n",
    "# --- 1. Model Architecture (Must match Implementation A exactly) ---\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(Actor, self).__init__()\n",
    "        # TF Architecture: 5 Conv layers taking 210x160x3 input\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2), nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Dummy pass to calculate flat size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 210, 160)\n",
    "            flat_size = self.features(dummy).shape[1]\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(flat_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# --- 2. GIF Generation Logic ---\n",
    "def generate_gif_impl_a():\n",
    "    print(f\"Loading Implementation A (High Res, 5-Layer CNN) from {ACTOR_PATH}...\")\n",
    "    \n",
    "    # 1. Setup Environment\n",
    "    # We use make_vec to match the training setup exactly\n",
    "    env = gym.make_vec(ENV_NAME, frameskip=4, num_envs=NUM_ENVS, vectorization_mode='sync')\n",
    "    \n",
    "    # 2. Instantiate Model\n",
    "    n_actions = env.single_action_space.n\n",
    "    actor = Actor(n_actions).to(DEVICE)\n",
    "    \n",
    "    # 3. Load Weights\n",
    "    if os.path.exists(ACTOR_PATH):\n",
    "        state_dict = torch.load(ACTOR_PATH, map_location=DEVICE)\n",
    "        actor.load_state_dict(state_dict)\n",
    "        print(\"Weights loaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Error: Checkpoint '{ACTOR_PATH}' not found!\")\n",
    "        return\n",
    "\n",
    "    actor.eval()\n",
    "    \n",
    "    # 4. Run Evaluation Loop\n",
    "    print(f\"Collecting {GIF_STEPS} frames...\")\n",
    "    \n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    # Buffer for GIF frames (uint8 to save RAM)\n",
    "    # Shape: (Steps, Envs, H, W, C) -> (1024, 16, 210, 160, 3)\n",
    "    obs_buf = np.zeros((GIF_STEPS, NUM_ENVS, 210, 160, 3), dtype=np.uint8)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step in range(GIF_STEPS):\n",
    "            # Store raw frame\n",
    "            obs_buf[step] = obs\n",
    "            \n",
    "            # Preprocess for Model: (B, H, W, C) -> Float -> Permute (B, C, H, W)\n",
    "            obs_norm = obs.astype(np.float32) / 255.0\n",
    "            obs_tensor = torch.tensor(obs_norm, dtype=torch.float32, device=DEVICE).permute(0, 3, 1, 2)\n",
    "            \n",
    "            # Get Action\n",
    "            logits = actor(obs_tensor)\n",
    "            dist = Categorical(logits=logits)\n",
    "            actions = dist.sample()\n",
    "            \n",
    "            # Step Env\n",
    "            obs, reward, terminated, truncated, _ = env.step(actions.cpu().numpy())\n",
    "\n",
    "    # 5. Save GIF\n",
    "    filename = \"ppo_impl_a_result.gif\"\n",
    "    print(f\"Saving GIF to {filename}...\")\n",
    "    \n",
    "    # Render the first environment only\n",
    "    to_render = obs_buf[:, 0, ...] \n",
    "    imageio.mimsave(filename, to_render, fps=30, loop=0)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1941ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_gif_impl_a()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
